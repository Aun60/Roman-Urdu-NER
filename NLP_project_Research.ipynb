{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5uR2QdlH539"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EM4tNHcIAv-"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets seqeval accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgolZaA1ICZN"
      },
      "outputs": [],
      "source": [
        "!pip install -q sklearn-crfsuite seqeval joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VVpHcp0IEX4",
        "outputId": "b6ebd079-12ce-45f8-8ec6-c2d0746f1815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded: train=5501, dev=300, test=300\n",
            "Fitting CRF on train set...\n",
            "\n",
            "CRF dev set classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DATE     0.6071    0.5667    0.5862        30\n",
            "       EVENT     1.0000    0.4800    0.6486        25\n",
            "         LOC     0.8953    0.7404    0.8105       104\n",
            "         ORG     0.5895    1.0000    0.7417        56\n",
            "         PER     0.8813    0.9800    0.9280       250\n",
            "     PRODUCT     0.8919    0.8919    0.8919        37\n",
            "\n",
            "   micro avg     0.8209    0.8765    0.8478       502\n",
            "   macro avg     0.8109    0.7765    0.7678       502\n",
            "weighted avg     0.8420    0.8765    0.8459       502\n",
            "\n",
            "\n",
            "Saved CRF model to crf_model.joblib in the notebook working directory.\n"
          ]
        }
      ],
      "source": [
        "import csv, os\n",
        "from sklearn_crfsuite import CRF\n",
        "from seqeval.metrics import classification_report\n",
        "import joblib\n",
        "\n",
        "TRAIN_CSV = \"roman_train_5500.csv\"\n",
        "DEV_CSV   = \"roman_dev_300.csv\"\n",
        "TEST_CSV  = \"roman_test_300.csv\"\n",
        "\n",
        "\n",
        "def read_csv_as_lists(csv_path):\n",
        "    \"\"\"\n",
        "    CSV expected columns: sentence_id, sentence, labels\n",
        "    sentence = tokens joined by space\n",
        "    labels   = BIO labels joined by space (aligned to tokens)\n",
        "    Returns list_of_tokens, list_of_labels\n",
        "    \"\"\"\n",
        "    sents = []\n",
        "    labs = []\n",
        "    with open(csv_path, encoding=\"utf8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            if 'sentence' in row and 'labels' in row:\n",
        "                sent = row['sentence'].strip()\n",
        "                labels = row['labels'].strip()\n",
        "            else:\n",
        "                vals = list(row.values())\n",
        "                if len(vals) >= 3:\n",
        "                    sent = vals[1].strip()\n",
        "                    labels = vals[2].strip()\n",
        "                else:\n",
        "                    continue\n",
        "            tokens = sent.split()\n",
        "            label_tokens = labels.split()\n",
        "            if len(tokens) != len(label_tokens):\n",
        "                # If alignment mismatch, try to best-effort fix: truncate to min length\n",
        "                m = min(len(tokens), len(label_tokens))\n",
        "                tokens = tokens[:m]\n",
        "                label_tokens = label_tokens[:m]\n",
        "            sents.append(tokens)\n",
        "            labs.append(label_tokens)\n",
        "    return sents, labs\n",
        "\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    f = {\n",
        "        'word.lower()': word.lower(),\n",
        "        'word.isupper()': word.isupper(),\n",
        "        'word.istitle()': word.istitle(),\n",
        "        'word.isdigit()': word.isdigit(),\n",
        "    }\n",
        "    if i>0:\n",
        "        w1=sent[i-1]\n",
        "        f.update({'-1:word.lower()':w1.lower(), '-1:istitle()':w1.istitle()})\n",
        "    else:\n",
        "        f['BOS']=True\n",
        "    if i<len(sent)-1:\n",
        "        w1=sent[i+1]\n",
        "        f.update({'+1:word.lower()':w1.lower(), '+1:istitle()':w1.istitle()})\n",
        "    else:\n",
        "        f['EOS']=True\n",
        "    f['prefix3']=word[:3]\n",
        "    f['suffix3']=word[-3:]\n",
        "    return f\n",
        "\n",
        "def sent2features(sent):\n",
        "    return [word2features(sent, i) for i in range(len(sent))]\n",
        "\n",
        "assert os.path.exists(TRAIN_CSV), f\"Train CSV not found: {TRAIN_CSV}\"\n",
        "assert os.path.exists(DEV_CSV), f\"Dev CSV not found: {DEV_CSV}\"\n",
        "assert os.path.exists(TEST_CSV), f\"Test CSV not found: {TEST_CSV}\"\n",
        "\n",
        "train_sents, train_labels = read_csv_as_lists(TRAIN_CSV)\n",
        "dev_sents, dev_labels = read_csv_as_lists(DEV_CSV)\n",
        "test_sents, test_labels = read_csv_as_lists(TEST_CSV)\n",
        "\n",
        "print(f\"Loaded: train={len(train_sents)}, dev={len(dev_sents)}, test={len(test_sents)}\")\n",
        "\n",
        "# Build feature matrices\n",
        "X_train = [sent2features(s) for s in train_sents]\n",
        "X_dev   = [sent2features(s) for s in dev_sents]\n",
        "\n",
        "# Train CRF\n",
        "crf = CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1, c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "print(\"Fitting CRF on train set...\")\n",
        "crf.fit(X_train, train_labels)\n",
        "\n",
        "# Predict & report\n",
        "y_pred = crf.predict(X_dev)\n",
        "print(\"\\nCRF dev set classification report:\\n\")\n",
        "print(classification_report(dev_labels, y_pred, digits=4))\n",
        "\n",
        "# Save model to disk\n",
        "joblib.dump(crf, \"crf_model.joblib\")\n",
        "print(\"\\nSaved CRF model to crf_model.joblib in the notebook working directory.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rwfD258JKHWX",
        "outputId": "38130e27-d0c1-45fe-abda-a7c07f9f495b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-1988790705.py:124: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2752' max='2752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2752/2752 13:12, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.782900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.182700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.085600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.063600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.039200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.045600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.025700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.035300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.033100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.018000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.022900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.037200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.042600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.018700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.024100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.017800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.011600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.019000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.019900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.016800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.027000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.019100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.019800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.020300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.024700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.022200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.017800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.011300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.009100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.017300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.014400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.014500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.020500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.015800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.013700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.015000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.012900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.009600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.025500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.014500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.011100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.017900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.016300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.011200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.012300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.012300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.013400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.015800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.015100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.014700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.013200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>0.009100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.007400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>0.018000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DEV RESULTS: {'eval_loss': 0.4220834970474243, 'eval_precision': 0.8380281690140845, 'eval_recall': 0.9054347826086957, 'eval_f1': 0.87042842215256, 'eval_runtime': 2.1608, 'eval_samples_per_second': 138.837, 'eval_steps_per_second': 17.586, 'epoch': 4.0}\n",
            "\n",
            "TEST RESULTS: {'test_loss': 0.44631582498550415, 'test_precision': 0.8341658341658341, 'test_recall': 0.9115720524017468, 'test_f1': 0.8711528429838289, 'test_runtime': 1.6114, 'test_samples_per_second': 186.176, 'test_steps_per_second': 23.582}\n",
            "\n",
            "Saved model to ./mbert_model\n"
          ]
        }
      ],
      "source": [
        "# MBERT\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification\n",
        ")\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "TRAIN_CSV = \"roman_train_5500.csv\"\n",
        "DEV_CSV   = \"roman_dev_300.csv\"\n",
        "TEST_CSV  = \"roman_test_300.csv\"\n",
        "\n",
        "\n",
        "def read_csv(csv_path):\n",
        "    sents, labs = [], []\n",
        "    with open(csv_path, encoding=\"utf8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            toks  = row[\"sentence\"].split()\n",
        "            labels = row[\"labels\"].split()\n",
        "            if len(toks) != len(labels):\n",
        "                m = min(len(toks), len(labels))\n",
        "                toks   = toks[:m]\n",
        "                labels = labels[:m]\n",
        "            sents.append(toks)\n",
        "            labs.append(labels)\n",
        "    return sents, labs\n",
        "\n",
        "train_sents, train_labels = read_csv(TRAIN_CSV)\n",
        "dev_sents, dev_labels = read_csv(DEV_CSV)\n",
        "test_sents, test_labels = read_csv(TEST_CSV)\n",
        "\n",
        "# Create label set\n",
        "label_set = sorted({l for seq in (train_labels + dev_labels + test_labels) for l in seq})\n",
        "if \"O\" in label_set:\n",
        "    label_set.remove(\"O\")\n",
        "label_set = [\"O\"] + label_set\n",
        "\n",
        "label_to_id = {l:i for i,l in enumerate(label_set)}\n",
        "id_to_label = {i:l for l,i in label_to_id.items()}\n",
        "\n",
        "MODEL = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
        "\n",
        "def encode_examples(sents, labels):\n",
        "    all_input_ids = []\n",
        "    all_attention = []\n",
        "    all_labels = []\n",
        "\n",
        "    for tokens, labs in zip(sents, labels):\n",
        "        tok = tokenizer(\n",
        "            tokens,\n",
        "            is_split_into_words=True,\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "        word_ids = tok.word_ids()\n",
        "        lab_ids = []\n",
        "\n",
        "        for w in word_ids:\n",
        "            if w is None:\n",
        "                lab_ids.append(-100)\n",
        "            else:\n",
        "                lab_ids.append(label_to_id[labs[w]])\n",
        "\n",
        "        all_input_ids.append(tok[\"input_ids\"])\n",
        "        all_attention.append(tok[\"attention_mask\"])\n",
        "        all_labels.append(lab_ids)\n",
        "\n",
        "    return Dataset.from_dict({\n",
        "        \"input_ids\": all_input_ids,\n",
        "        \"attention_mask\": all_attention,\n",
        "        \"labels\": all_labels\n",
        "    })\n",
        "\n",
        "train_ds = encode_examples(train_sents, train_labels)\n",
        "dev_ds   = encode_examples(dev_sents, dev_labels)\n",
        "test_ds  = encode_examples(test_sents, test_labels)\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    MODEL,\n",
        "    num_labels=len(label_set),\n",
        "    id2label=id_to_label,\n",
        "    label2id=label_to_id\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    preds = np.argmax(preds, axis=2)\n",
        "\n",
        "    true_labels = [[id_to_label[l] for l in lab_row if l != -100] for lab_row in labels]\n",
        "    true_preds  = [[id_to_label[p] for p,l in zip(pred_row, lab_row) if l != -100]\n",
        "                   for pred_row, lab_row in zip(preds, labels)]\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision_score(true_labels, true_preds),\n",
        "        \"recall\": recall_score(true_labels, true_preds),\n",
        "        \"f1\": f1_score(true_labels, true_preds)\n",
        "    }\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"mbert_out1\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    logging_steps=50,\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=dev_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nDEV RESULTS:\", trainer.evaluate())\n",
        "\n",
        "test_out = trainer.predict(test_ds)\n",
        "print(\"\\nTEST RESULTS:\", test_out.metrics)\n",
        "\n",
        "trainer.save_model(\"mbert_model\")\n",
        "print(\"\\nSaved model to ./mbert_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gojP_TnYNrX5",
        "outputId": "b170ed8d-72f2-412b-cd66-1191c44a29c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DATE     0.5730    0.7183    0.6375        71\n",
            "       EVENT     1.0000    1.0000    1.0000        37\n",
            "         LOC     0.9306    0.7090    0.8048       189\n",
            "         ORG     0.5588    1.0000    0.7170        57\n",
            "         PER     0.9089    0.9961    0.9505       511\n",
            "     PRODUCT     0.6812    0.9216    0.7833        51\n",
            "\n",
            "   micro avg     0.8342    0.9116    0.8712       916\n",
            "   macro avg     0.7754    0.8908    0.8155       916\n",
            "weighted avg     0.8566    0.9116    0.8743       916\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "preds = np.argmax(test_out.predictions, axis=2)\n",
        "\n",
        "true_labels = []\n",
        "true_preds  = []\n",
        "\n",
        "for pred_row, label_row in zip(preds, test_out.label_ids):\n",
        "    t_labels = []\n",
        "    t_preds  = []\n",
        "    for p,l in zip(pred_row, label_row):\n",
        "        if l != -100:\n",
        "            t_labels.append(id_to_label[l])\n",
        "            t_preds.append(id_to_label[p])\n",
        "    true_labels.append(t_labels)\n",
        "    true_preds.append(t_preds)\n",
        "\n",
        "print(classification_report(true_labels, true_preds, digits=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5F1oAWWjNweE",
        "outputId": "d386a33a-1b37-4f77-c36b-312b309c78ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-3885174628.py:124: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1252' max='1252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1252/1252 13:13, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.327100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.327000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.130300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.075900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.060800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.041300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.032900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.045100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.039700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.031100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.033900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.026200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.023200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.023400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.022200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.028100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.022100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.023800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.027000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.022300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.020600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.019000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.024500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.013500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.021200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DEV RESULTS: {'eval_loss': 0.33201727271080017, 'eval_precision': 0.8770114942528736, 'eval_recall': 0.9585427135678392, 'eval_f1': 0.9159663865546218, 'eval_runtime': 0.5892, 'eval_samples_per_second': 509.182, 'eval_steps_per_second': 64.496, 'epoch': 4.0}\n",
            "\n",
            "TEST RESULTS: {'test_loss': 0.3613591492176056, 'test_precision': 0.8760045924225028, 'test_recall': 0.9670468948035488, 'test_f1': 0.919277108433735, 'test_runtime': 0.6394, 'test_samples_per_second': 469.179, 'test_steps_per_second': 59.429}\n",
            "\n",
            "Saved model to ./xlmr_model\n"
          ]
        }
      ],
      "source": [
        "#XLM-RoBERTa NER\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification\n",
        ")\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "TRAIN_CSV = \"roman_train_5500.csv\"\n",
        "DEV_CSV   = \"roman_dev_300.csv\"\n",
        "TEST_CSV  = \"roman_test_300.csv\"\n",
        "\n",
        "\n",
        "def read_csv(csv_path):\n",
        "    sents, labs = [], []\n",
        "    with open(csv_path, encoding=\"utf8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            toks  = row[\"sentence\"].split()\n",
        "            labels = row[\"labels\"].split()\n",
        "            if len(toks) != len(labels):\n",
        "                m = min(len(toks), len(labels))\n",
        "                toks   = toks[:m]\n",
        "                labels = labels[:m]\n",
        "            sents.append(toks)\n",
        "            labs.append(labels)\n",
        "    return sents, labs\n",
        "\n",
        "train_sents, train_labels = read_csv(TRAIN_CSV)\n",
        "dev_sents, dev_labels = read_csv(DEV_CSV)\n",
        "test_sents, test_labels = read_csv(TEST_CSV)\n",
        "\n",
        "# Build label set\n",
        "label_set = sorted({l for seq in (train_labels + dev_labels + test_labels) for l in seq})\n",
        "if \"O\" in label_set:\n",
        "    label_set.remove(\"O\")\n",
        "label_set = [\"O\"] + label_set\n",
        "\n",
        "label_to_id = {l:i for i,l in enumerate(label_set)}\n",
        "id_to_label = {i:l for l,i in label_to_id.items()}\n",
        "\n",
        "MODEL_NAME = \"xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "def encode_examples(sents, labels):\n",
        "    all_input_ids = []\n",
        "    all_attention = []\n",
        "    all_labels = []\n",
        "\n",
        "    for tokens, labs in zip(sents, labels):\n",
        "        tok = tokenizer(\n",
        "            tokens,\n",
        "            is_split_into_words=True,\n",
        "            truncation=True,\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "        word_ids = tok.word_ids()\n",
        "        aligned = []\n",
        "        for w in word_ids:\n",
        "            if w is None:\n",
        "                aligned.append(-100)\n",
        "            else:\n",
        "                aligned.append(label_to_id[labs[w]])\n",
        "\n",
        "        all_input_ids.append(tok[\"input_ids\"])\n",
        "        all_attention.append(tok[\"attention_mask\"])\n",
        "        all_labels.append(aligned)\n",
        "\n",
        "    return Dataset.from_dict({\n",
        "        \"input_ids\": all_input_ids,\n",
        "        \"attention_mask\": all_attention,\n",
        "        \"labels\": all_labels\n",
        "    })\n",
        "\n",
        "train_ds = encode_examples(train_sents, train_labels)\n",
        "dev_ds   = encode_examples(dev_sents, dev_labels)\n",
        "test_ds  = encode_examples(test_sents, test_labels)\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(label_set),\n",
        "    id2label=id_to_label,\n",
        "    label2id=label_to_id\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    preds, labels = eval_pred\n",
        "    preds = np.argmax(preds, axis=2)\n",
        "\n",
        "    true_labels = [[id_to_label[l] for l in lab_row if l != -100] for lab_row in labels]\n",
        "    true_preds  = [[id_to_label[p] for p,l in zip(pred_row, lab_row) if l != -100]\n",
        "                   for pred_row, lab_row in zip(preds, labels)]\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision_score(true_labels, true_preds),\n",
        "        \"recall\": recall_score(true_labels, true_preds),\n",
        "        \"f1\": f1_score(true_labels, true_preds)\n",
        "    }\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"xlmr_out1\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=4,\n",
        "    logging_steps=50,\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=dev_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\nDEV RESULTS:\", trainer.evaluate())\n",
        "\n",
        "test_out = trainer.predict(test_ds)\n",
        "print(\"\\nTEST RESULTS:\", test_out.metrics)\n",
        "\n",
        "trainer.save_model(\"xlmr_model\")\n",
        "print(\"\\nSaved model to ./xlmr_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCzm6q4xNwtC",
        "outputId": "6eb25076-beeb-4d48-b0ec-9c8b8df7d241"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DATE     0.5909    0.7222    0.6500        54\n",
            "       EVENT     1.0000    0.9730    0.9863        37\n",
            "         LOC     0.9444    0.9551    0.9497       178\n",
            "         ORG     0.5377    1.0000    0.6994        57\n",
            "         PER     0.9928    0.9976    0.9952       412\n",
            "     PRODUCT     0.7246    0.9804    0.8333        51\n",
            "\n",
            "   micro avg     0.8760    0.9670    0.9193       789\n",
            "   macro avg     0.7984    0.9380    0.8523       789\n",
            "weighted avg     0.9045    0.9670    0.9290       789\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "preds = np.argmax(test_out.predictions, axis=2)\n",
        "\n",
        "true_labels = []\n",
        "true_preds  = []\n",
        "\n",
        "for pred_row, label_row in zip(preds, test_out.label_ids):\n",
        "    t_labels = []\n",
        "    t_preds  = []\n",
        "    for p,l in zip(pred_row, label_row):\n",
        "        if l != -100:\n",
        "            t_labels.append(id_to_label[l])\n",
        "            t_preds.append(id_to_label[p])\n",
        "    true_labels.append(t_labels)\n",
        "    true_preds.append(t_preds)\n",
        "\n",
        "print(classification_report(true_labels, true_preds, digits=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71OegGNHN_V1",
        "outputId": "af2a5887-7cfb-428a-ac9f-6190e4869579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer from: bert-base-multilingual-cased\n",
            "Loading fine-tuned model from: ./mbert_model\n",
            "\n",
            "** mBERT NER tester **\n",
            "Type a sentence and press Enter. Type 'exit' or empty line to quit.\n",
            "\n",
            "Enter sentence > Kal subha Areeba Khan Pindi Cantt se office ja rhi thi.\n",
            "\n",
            "Input words:\n",
            "00: Kal                   --> O\n",
            "01: subha                 --> O\n",
            "02: Areeba                --> B-PER\n",
            "03: Khan                  --> I-PER\n",
            "04: Pindi                 --> B-LOC\n",
            "05: Cantt                 --> I-LOC\n",
            "06: se                    --> O\n",
            "07: office                --> O\n",
            "08: ja                    --> O\n",
            "09: rhi                   --> O\n",
            "10: thi.                  --> O\n",
            "\n",
            "Detected entities (count=2):\n",
            " - 'Areeba Khan' \t label=PER \t span=(2,3)\n",
            " - 'Pindi Cantt' \t label=LOC \t span=(4,5)\n",
            "\n",
            "Enter sentence > Aaj Bilal ne apna Nikon camera repair krawaya\n",
            "\n",
            "Input words:\n",
            "00: Aaj                   --> O\n",
            "01: Bilal                 --> B-PER\n",
            "02: ne                    --> O\n",
            "03: apna                  --> O\n",
            "04: Nikon                 --> B-PRODUCT\n",
            "05: camera                --> I-PRODUCT\n",
            "06: repair                --> O\n",
            "07: krawaya               --> O\n",
            "\n",
            "Detected entities (count=2):\n",
            " - 'Bilal' \t label=PER \t span=(1,1)\n",
            " - 'Nikon camera' \t label=PRODUCT \t span=(4,5)\n",
            "\n",
            "Enter sentence > Ali ne kapre change kiye\n",
            "\n",
            "Input words:\n",
            "00: Ali                   --> B-PER\n",
            "01: ne                    --> O\n",
            "02: kapre                 --> O\n",
            "03: change                --> O\n",
            "04: kiye                  --> O\n",
            "\n",
            "Detected entities (count=1):\n",
            " - 'Ali' \t label=PER \t span=(0,0)\n",
            "\n",
            "Enter sentence > Ali ne kapre Change kiye\n",
            "\n",
            "Input words:\n",
            "00: Ali                   --> B-PER\n",
            "01: ne                    --> O\n",
            "02: kapre                 --> O\n",
            "03: Change                --> O\n",
            "04: kiye                  --> O\n",
            "\n",
            "Detected entities (count=1):\n",
            " - 'Ali' \t label=PER \t span=(0,0)\n",
            "\n",
            "Enter sentence > Ali Faisalabad gaya ARYNEWS pr news dekhne\n",
            "\n",
            "Input words:\n",
            "00: Ali                   --> B-PER\n",
            "01: Faisalabad            --> B-LOC\n",
            "02: gaya                  --> O\n",
            "03: ARYNEWS               --> B-ORG\n",
            "04: pr                    --> O\n",
            "05: news                  --> O\n",
            "06: dekhne                --> O\n",
            "\n",
            "Detected entities (count=3):\n",
            " - 'Ali' \t label=PER \t span=(0,0)\n",
            " - 'Faisalabad' \t label=LOC \t span=(1,1)\n",
            " - 'ARYNEWS' \t label=ORG \t span=(3,3)\n",
            "\n",
            "Enter sentence > exit\n",
            "Exiting.\n"
          ]
        }
      ],
      "source": [
        "# TESTING\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "import torch\n",
        "\n",
        "BASE_MODEL = \"bert-base-multilingual-cased\"   # original tokenizer source\n",
        "MODEL_DIR = \"./mbert_model\"                   # fine-tuned model weights folder\n",
        "\n",
        "print(\"Loading tokenizer from:\", BASE_MODEL)\n",
        "print(\"Loading fine-tuned model from:\", MODEL_DIR)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR)\n",
        "\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Build id2label mapping\n",
        "if hasattr(model.config, \"id2label\") and model.config.id2label:\n",
        "    id2label = model.config.id2label\n",
        "else:\n",
        "    id2label = {i: f\"LABEL_{i}\" for i in range(model.config.num_labels)}\n",
        "\n",
        "def predict_sentence_entities(sentence, threshold_softmax=None):\n",
        "    \"\"\"\n",
        "    Predict token labels for a single sentence and return entity spans.\n",
        "    Returns: dict with tokens, words, predicted_word_labels, entities list\n",
        "    \"\"\"\n",
        "    # Split into words for is_split_into_words=True alignment\n",
        "    words = sentence.split()\n",
        "    # Tokenize with word-level mapping and return tensors\n",
        "    enc = tokenizer(words,\n",
        "                    is_split_into_words=True,\n",
        "                    return_offsets_mapping=True,\n",
        "                    truncation=True,\n",
        "                    max_length=256,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True)\n",
        "    input_ids = enc[\"input_ids\"].to(device)\n",
        "    attention_mask = enc[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = out.logits  # shape: (1, seq_len, num_labels)\n",
        "        probs = None\n",
        "        if threshold_softmax is not None:\n",
        "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "    pred_ids = torch.argmax(logits, dim=-1).squeeze().cpu().tolist()\n",
        "\n",
        "    # Build mapping from tokens->word index\n",
        "    word_ids = enc.word_ids(batch_index=0)  # list of word_id per token (None for special tokens)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze().cpu().tolist())\n",
        "\n",
        "    # Collect predicted label ids for each token, then pick first token label for the word\n",
        "    word_to_subtoken_labels = {}\n",
        "    token_index = 0\n",
        "    for idx, wid in enumerate(word_ids):\n",
        "        if wid is None:\n",
        "            continue\n",
        "        lbl = pred_ids[idx]\n",
        "        if wid not in word_to_subtoken_labels:\n",
        "            word_to_subtoken_labels[wid] = []\n",
        "        word_to_subtoken_labels[wid].append((idx, lbl))\n",
        "        token_index += 1\n",
        "\n",
        "    # Decide per-word label: use first subtoken's predicted label\n",
        "    word_labels = []\n",
        "    for wid in range(len(words)):\n",
        "        if wid in word_to_subtoken_labels:\n",
        "            first_label_id = word_to_subtoken_labels[wid][0][1]\n",
        "            word_labels.append(first_label_id)\n",
        "        else:\n",
        "            # If a word had no tokens (unlikely), mark as O if 'O' exists\n",
        "            # fall back to label id 0\n",
        "            word_labels.append(0)\n",
        "\n",
        "    # Convert label ids to label strings\n",
        "    word_label_names = [id2label.get(lid, str(lid)) for lid in word_labels]\n",
        "\n",
        "    # Now extract entities based on BIO scheme (B-XXX, I-XXX, O)\n",
        "    entities = []\n",
        "    current_ent = None  # (label, start_word_idx, end_word_idx, tokens list)\n",
        "    for i, lab in enumerate(word_label_names):\n",
        "        if lab == \"O\" or lab == \"o\":\n",
        "            if current_ent is not None:\n",
        "                # close entity\n",
        "                label, start, toks = current_ent\n",
        "                entities.append({\n",
        "                    \"label\": label,\n",
        "                    \"start_word\": start,\n",
        "                    \"end_word\": i-1,\n",
        "                    \"text\": \" \".join(words[start:i])\n",
        "                })\n",
        "                current_ent = None\n",
        "            continue\n",
        "\n",
        "        # label like B-PER or I-LOC etc.\n",
        "        if lab.startswith(\"B-\"):\n",
        "            if current_ent is not None:\n",
        "                # close previous\n",
        "                label_prev, start_prev, toks_prev = current_ent\n",
        "                entities.append({\n",
        "                    \"label\": label_prev,\n",
        "                    \"start_word\": start_prev,\n",
        "                    \"end_word\": i-1,\n",
        "                    \"text\": \" \".join(words[start_prev:i])\n",
        "                })\n",
        "            ent_label = lab.split(\"-\", 1)[1]\n",
        "            current_ent = (ent_label, i, [words[i]])\n",
        "        elif lab.startswith(\"I-\"):\n",
        "            ent_label = lab.split(\"-\", 1)[1]\n",
        "            if current_ent is None:\n",
        "                # treat I- as B-\n",
        "                current_ent = (ent_label, i, [words[i]])\n",
        "            else:\n",
        "                # continue only if same label\n",
        "                if current_ent[0] == ent_label:\n",
        "                    pass  # continue\n",
        "                else:\n",
        "                    # close previous and start new\n",
        "                    label_prev, start_prev, toks_prev = current_ent\n",
        "                    entities.append({\n",
        "                        \"label\": label_prev,\n",
        "                        \"start_word\": start_prev,\n",
        "                        \"end_word\": i-1,\n",
        "                        \"text\": \" \".join(words[start_prev:i])\n",
        "                    })\n",
        "                    current_ent = (ent_label, i, [words[i]])\n",
        "        else:\n",
        "            # Unknown format (maybe label without B-/I-). If it's like 'PER' assume B-\n",
        "            if \"-\" not in lab:\n",
        "                # close previous if exists\n",
        "                if current_ent is not None:\n",
        "                    label_prev, start_prev, toks_prev = current_ent\n",
        "                    entities.append({\n",
        "                        \"label\": label_prev,\n",
        "                        \"start_word\": start_prev,\n",
        "                        \"end_word\": i-1,\n",
        "                        \"text\": \" \".join(words[start_prev:i])\n",
        "                    })\n",
        "                current_ent = (lab, i, [words[i]])\n",
        "            else:\n",
        "                # fallback treat as O\n",
        "                if current_ent is not None:\n",
        "                    label_prev, start_prev, toks_prev = current_ent\n",
        "                    entities.append({\n",
        "                        \"label\": label_prev,\n",
        "                        \"start_word\": start_prev,\n",
        "                        \"end_word\": i-1,\n",
        "                        \"text\": \" \".join(words[start_prev:i])\n",
        "                    })\n",
        "                    current_ent = None\n",
        "\n",
        "    # Close last entity if present\n",
        "    if current_ent is not None:\n",
        "        label, start, toks = current_ent\n",
        "        entities.append({\n",
        "            \"label\": label,\n",
        "            \"start_word\": start,\n",
        "            \"end_word\": len(words)-1,\n",
        "            \"text\": \" \".join(words[start:len(words)])\n",
        "        })\n",
        "\n",
        "    return {\n",
        "        \"words\": words,\n",
        "        \"word_labels\": word_label_names,\n",
        "        \"entities\": entities\n",
        "    }\n",
        "\n",
        "def pretty_print_result(res):\n",
        "    print(\"\\nInput words:\")\n",
        "    for i,w in enumerate(res[\"words\"]):\n",
        "        print(f\"{i:02d}: {w:20}  --> {res['word_labels'][i]}\")\n",
        "    print(\"\\nDetected entities (count={}):\".format(len(res[\"entities\"])))\n",
        "    for e in res[\"entities\"]:\n",
        "        print(f\" - {e['text']!r} \\t label={e['label']} \\t span=({e['start_word']},{e['end_word']})\")\n",
        "    print(\"\")\n",
        "\n",
        "# Interactive loop\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n** mBERT NER tester **\")\n",
        "    print(\"Type a sentence and press Enter. Type 'exit' or empty line to quit.\\n\")\n",
        "    while True:\n",
        "        s = input(\"Enter sentence(Identifying Entities Totally Depend on the Dataset. ) > \").strip()\n",
        "        if s.lower() in (\"exit\", \"quit\", \"\"):\n",
        "            print(\"Exiting.\")\n",
        "            break\n",
        "        try:\n",
        "            r = predict_sentence_entities(s)\n",
        "            pretty_print_result(r)\n",
        "        except Exception as ex:\n",
        "            print(\"Error during prediction:\", ex)\n",
        "            print(\"Make sure MODEL_DIR points to a valid fine-tuned mBERT model folder.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-R7aVG6lS3-w"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
